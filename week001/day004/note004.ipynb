{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"note004.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AQdnZ4t-PKW6","colab_type":"text"},"source":["4쪽 :  \n","Features : 통계학에선 설명변수  \n","\n","perceptron : IPO(input process output)  \n","\n","sigmoid(통계학에선 rogistic function) : 모든 숫자를 0 < val < 1 사이로 수렴시킴  \n","\n","sigmoid에서 good쪽 edge의 가중치값이 더 커서 1의 값으로 예측"]},{"cell_type":"markdown","metadata":{"id":"mjKXXUb-PKW6","colab_type":"text"},"source":["5쪽 :  \n","sigmoid도 activation중에 하나  \n","\n","상, 중, 하 : 상일 확률 0.9, 아닐 확률 0.1  \n","중일 확률 0.8, , 아닐 확률 0.2  \n","하일 확률 0.55, 아닐 확률 0.45 라서   \n","softmax에 대입, 그리고 argmax를 통해서 pred를 구함  \n","\n","pred를 구하기 위해서 argmax사용  \n","\n","evaluation : \n","https://www.quora.com/What-is-the-difference-between-evaluation-and-validation-in-machine-learning"]},{"cell_type":"markdown","metadata":{"id":"5mTgzWvRPKW7","colab_type":"text"},"source":["10쪽 :  \n","주황색 node에 activation을 적용 안하고 숫자 값 예측하기 == 좀 더 어렵다.\n","== 예측은 신의 영역이다 ㄸㄹㄹ  \n","중간에는 activation을 적용해도 가중치를 적용해서 학습해서 상관 없다...  "]},{"cell_type":"markdown","metadata":{"id":"efPwjWy0PKW7","colab_type":"text"},"source":["13쪽 :   \n","정규분포의random value  \n","정규분포의 형태를 따른다 생각하고 값을 선택\n","\n","정규분포+/-2시그마내random value  \n","weight의 값이 -2와 2사이의 값일 확률이 주로 높다는 사실을 바탕으로...  \n","-2와 2의 값을 선택\n","\n","Weight(784, 8), Weight(8, 3) : 벡터의 내적 : 행렬의 곱  \n","다음 layer를 넘어갈 때마다 행렬 곱하기를 한다.(MXNet은 병렬처리 지원, tensorflow는 아직 병렬 지원 안해줌 ㄸㄹㄹ)"]},{"cell_type":"markdown","metadata":{"id":"K_kX5ZMvPKW8","colab_type":"text"},"source":["자주쓰는 activation함수  \n","sigmoid, Relud, tanh(hyperbolic tangent)"]},{"cell_type":"markdown","metadata":{"id":"RE8S-sRDPKW8","colab_type":"text"},"source":["16쪽 :  \n","activation을 중간에 쓰고 말고는 내가 정해야 함(supervised..)"]},{"cell_type":"markdown","metadata":{"id":"gj9NsadoPKW9","colab_type":"text"},"source":["<img src=\"loss_function.png\">"]},{"cell_type":"markdown","metadata":{"id":"aDuyZQD3PKW9","colab_type":"text"},"source":["17쪽 :  \n","학습 : 가중치에 대한 오차 최소화과정  \n","오차합 == loss == cost  \n","수정 : backpropagation과정으로 weight값을 수정함    \n","backpropagation공식에서 미분값 앞의 계수를 learning rate라고 생각하자.  \n","\n","\n","global minimum을 찾는 것이 중요하다.  \n","learning rate : 손실함수에서 최솟값으로 다가가는 속도  \n","learning rate가 너무 크면 : global minimum에 도달하기 힘들다. (정밀도가 떨어진다.)   \n","learning rate가 너무 작으면 : 느리다, local minimum에 빠질 가능성이 있다.  \n","==> learning rate가 적당히 커야 local minimum을 벗어날 수 있다.  "]},{"cell_type":"markdown","metadata":{"id":"TebkLiJIPKW-","colab_type":"text"},"source":["22쪽 :   \n","random하게 node를 끊어서 overfitting을 막음  \n","deep learning에서 정규화 방법 : drop out"]},{"cell_type":"markdown","metadata":{"id":"MVu9fC8YPKW-","colab_type":"text"},"source":["23쪽 :  \n","\"input 그림을 기준으로 긴 머리 여성분의 이미지\"를 일반화한 것이 \"흑백사진에 있는(Face에 있는) 긴 머리 여성분 이미지\" : 일반화가 되어서 train data에 없는 \"긴 머리 여성분\"을 분류할 수 있게된다."]},{"cell_type":"markdown","metadata":{"id":"_DZANv21PKW_","colab_type":"text"},"source":["57쪽 :  \n","Autoencoder 왜 씀? 데이터에 outlier를 찾기 위해서 사용한다.  \n","Reconstruction Error= Input Vector -Output Vector 값이 큰 값을  "]},{"cell_type":"markdown","metadata":{"id":"0ocBIKfzPKW_","colab_type":"text"},"source":["59쪽 :  \n","PCA는 sparse하면 학습을 못함 & 정규분포를 가정함"]}]}