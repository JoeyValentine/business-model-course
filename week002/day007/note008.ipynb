{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"note008.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GxWcfhapfvxb","colab_type":"text"},"source":["# 목적 : 오차가 최소가 되도록하는 weight와 bias를 구하고 싶다."]},{"cell_type":"markdown","metadata":{"id":"HKjoBvsqfvxc","colab_type":"text"},"source":["### 7쪽  \n","유전 형질이 평균에 가까워진다.(유전 형질이 평균에 회귀한다.) == regression단어의 유래"]},{"cell_type":"markdown","metadata":{"id":"pKI3rvHrfvxd","colab_type":"text"},"source":["### 8쪽  \n","alpha, beta는 미지수  \n","SSE는 alpha와 beta에 대한 2차함수니까, 미분해서 0이 되는 경우가 최솟값임"]},{"cell_type":"markdown","metadata":{"id":"B-7-_aVZfvxd","colab_type":"text"},"source":["### 10쪽  \n","hat은 예측값  \n","일반적인 회귀모형 𝑌 = 𝛼 + 𝛽𝑋 + 𝜖 으로 표현할 경우, 예측하는 경우 선형이라서 범위를 벗어나게 됨  \n","좌변의 범위는 (0, 1) 이고 우변은 (-inf, inf)인 문제가 발생. 이 문제를 해결하기 위해 로지스틱 함수를 사용함\n","\n","\n","회귀모형에서 종속변수 Y=0,1 과 같이 Binary Factor 변수일 경우 문제가 발생 : 분류문제  \n","beta hat을 수학적으로 계산할 수 없어(식으로 나타낼 수 없어서)"]},{"cell_type":"markdown","metadata":{"id":"NC92G-Bkfvxe","colab_type":"text"},"source":["### 11쪽  \n","\n","SSE가 Convex 하지 않아 대안으로 아래의 Cross Entropy 함수를 사용함  \n","SSE가 울룩불룩함 == local minima에 빠질 가능성이 있음  \n","\n","H(X) = 1 / (1 + exp(-beta0 - beta1 * xi))  \n","cost(W) = sum(-yi * log(H(xi)) - (1-yi) * log(1-H(xi))), yi가 1또는 0일 때 한 쪽이 사라짐  \n","\n","이 함수는 y = 1 일 때, H(X) 가 커지면 함수 값이 작아지고, H(X)가 작아지면 무한대가 됨  \n","또한, y = 0 일 때, H(X) 가 작아지면 함수 값이 작아지고, H(X)가 커지면 무한대가 됨  \n","==> 2개 분류하는 문제에서,   \n","y=1일 때, H(X)==1에 가까우면, H(X)가 커져서 함숫값이 작아지고   \n","linear regression의 경우 SSE는 convex함    \n","\n","### cost함수 조건  \n","1. convexity  \n","2. 오차가 크면 함숫값이 커짐  \n","3. 오차가 작으면 함숫값이 작아짐\n","\n","는 저 위의 조건을 모두 만족하도록 일부러 12쪽에 있는 cost(W)를 만들었다.  \n","\n","### Cross Entropy 함수 : 이 함수가 convex함을 증명할 수 있음, 그래서 사용하게 됨\n","이 식에 “-1”을 곱한 식이 Cross Entropy 함수다. == 값이 크면 오차가 큰거고, 작으면 오차가 작은것을 나타내기 위해서 음수를 곱함  "]},{"cell_type":"markdown","metadata":{"id":"mM_sqw3Vfvxf","colab_type":"text"},"source":["### 12쪽 \n","pi == H(X)  \n","최대가능도 함수(Maximum Likelihood function)  \n","cost를 최소로 하기 위해서 이 식에 “-1”을 곱한 식이 Cross Entropy 함수다.  \n","추측값을 hat으로 표현  \n","\n","이 함수를 최대화 하는 𝑝𝑖 가 가장 관측값을 잘 설명하는 추정값이라고 생각하는 것이 MLE 추정(Maximum Likelihood\n","Estimation)이다.\n","값이 크면 예측을 잘 했다. 값이 작으면 예측을 잘 못했다.  \n","\n","f(y0) : y값이 y0일 확률   \n","\n","의미상으론  \n","X에 의해서 Y값이 결정되므로 Y끼리는 독립   \n","\n","Joint pdf 를 구해서 그 값이 각각의 확률 곱이랑 같음을 보이면 독립임을 보일 수 있음   \n","f(x,y) = f1(x) * f2(y)"]},{"cell_type":"markdown","metadata":{"id":"ufbYM9j8fvxg","colab_type":"text"},"source":["### 13쪽   \n","연립방정식을 풀어서 w와 b를 구하는 식을 구함  \n","S : 1 / (1 + exp(-x))  "]},{"cell_type":"markdown","metadata":{"id":"QEQsGaETf1p-","colab_type":"text"},"source":["[최대 가능도 함수](http://www.deeplearningbook.org/contents/mlp.html)\n","\n","[로그 최대 가능도 함수를 사용하는 이유](http://blog.naver.com/PostView.nhn?blogId=lucifer246&logNo=178208039)\n","\n"]}]}