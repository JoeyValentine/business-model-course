{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"note009.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"minIWng9NsDx","colab_type":"code","outputId":"55b86f6a-1e7d-448f-c962-f3a4c9c74ddb","executionInfo":{"status":"ok","timestamp":1562731374920,"user_tz":-540,"elapsed":784,"user":{"displayName":"김민우","photoUrl":"","userId":"05585706814939375266"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["from functools import reduce\n","\n","# y = ax + b 에서 a와 b를 추측하는 함수\n","x = [1, 2, 3]\n","y = [1, 2, 3]\n","\n","n_x = len(x)\n","n_y = len(y)\n","\n","x_bar = sum(x) / n_x\n","y_bar = sum(y) / n_y\n","\n","# 선형일 때는 Least square method & gradient descent 둘 다 가능\n","# 하지만 비선형일때는 Least square method로 \n","# 해를 못 구할 가능성이 높아서 gradient descent로 구함 (logistic regression)\n","\n","# Least square method로 구하기 == alpha, beta에 대한 해를 구할 수 있을때만 사용 가능\n","\n","beta = reduce(lambda x,y : x+y, [(xi - x_bar) * (yi - y_bar) for xi, yi in zip(x,y)]) \\\n","       / reduce(lambda x,y : x+y, [(xi - x_bar)**2 for xi in x])\n","alpha = y_bar - beta * x_bar\n","print(alpha, beta)\n","\n","# sum1 = sum2 = 0\n","# for i in range(n_x):\n","#   sum1 = sum1 + (x[i] - x_bar)**2\n","#   sum2 = sum2 + (x[i] - x_bar) * (y[i] - y_bar)\n","\n","# beta = sum2 / sum1\n","# alpha = y_bar - beta * x_bar\n","\n","# print(alpha, beta)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["0.0 1.0\n","0.0 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2Gi4mY87FO1O","colab_type":"text"},"source":["[왜 gradient descent 방식으로 해결하는가? 1](https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent)  \n","[왜 gradient descent 방식으로 해결하는가? 2](https://stats.stackexchange.com/questions/212619/why-is-gradient-descent-required)  \n","[왜 gradient descent 방식으로 해결하는가? 3](https://stats.stackexchange.com/questions/278755/why-use-gradient-descent-for-linear-regression-when-a-closed-form-math-solution)"]},{"cell_type":"code","metadata":{"id":"VMz4vgJTPLBi","colab_type":"code","colab":{}},"source":["# SSE에서 미분해서 도함수를 구함\n","def deriv(alpha, beta):\n","  sum1 = sum2 = 0\n","  for i in range(n_x):\n","    sum1 = sum1 - 2 * (y[i] - alpha - beta * x[i])\n","    sum2 = sum2 - 2 * (y[i] - alpha - beta * x[i]) * x[i]\n","  return sum1, sum2\n","\n","# gradient decent 방법으로 임의의 값에서 출발\n","alpha = beta = 0.5 \n","lamda = 0.1\n","\n","for i in range(100):\n","  alpha = alpha - lamda * deriv(alpha, beta)[0]\n","  beta = beta - lamda * deriv(alpha, beta)[1]\n","  print(alpha, beta)\n","\n","# 근사치로 해를 구함\n","# local minimum이 global minimum인지 알 수 없다."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvgaLtxqUCyr","colab_type":"text"},"source":["### 10쪽  \n","𝑌 = 1/ (1 + exp(−(𝛽0 + 𝛽1𝑥1 + ⋯ + 𝛽𝑘𝑥𝑘 + 𝜖))  )  \n","입실론은 오차를 의미함  \n","== X로는 구할 수 없는 오차를 의미함  \n","Y는 X말고도 많은 feature에 의해서 영향을 받음  \n","\n","Y hat은 예측값이며 X의 함수이기 때문에 H(X)로 표현  "]},{"cell_type":"markdown","metadata":{"id":"gCHMApt-UEOq","colab_type":"text"},"source":["### 14쪽\n","activation : 활성함수 : 일정 자극 이상(역치) 주어져야 다음 뉴런으로 신경물질을 전달  \n","sigmoid에서 일정 수준 이상 자극을 주면 "]},{"cell_type":"markdown","metadata":{"id":"GSsQexhnXaIO","colab_type":"text"},"source":["### 15쪽  \n","neural network로 or and xor를 해결하려고 했다.   \n","=> x1, x2 만으로는 해결할 수 없음  \n","\n","해결책  \n","x1, x2에서 새로운 변수 h1, h2를 구해서 문제를 해결하려 함  "]},{"cell_type":"markdown","metadata":{"id":"7aSrgvl6duNU","colab_type":"text"},"source":["### 17쪽  \n","여러 hidden layer가 존재하는 경우 어떻게 전부 수정할 수 있을까??? \n"]},{"cell_type":"markdown","metadata":{"id":"ZjBvjJMrgpA0","colab_type":"text"},"source":["### 18쪽  \n","cost함수 미분할 때 편할라고 1/2 추가함 1/2 Cost의 최솟값이나 Cost의 최솟값 구하는 것은 같으므로 괜찮음  \n","\n","gradient descent방법으로 W1에서 W2를 갱신  \n","\n","W2에서 W1을 갱신하려면,,,"]},{"cell_type":"markdown","metadata":{"id":"6oU-O61EjdQ8","colab_type":"text"},"source":["### 21쪽   \n","bias라고 표현하는 이유...?  고쳐주는 값  "]},{"cell_type":"markdown","metadata":{"id":"-7y7_riolY8N","colab_type":"text"},"source":["### 22쪽  \n","색이 진할수록 weight가 진한 것을 의미  \n","back propagation을 진행하면서 gradient 값이 계속 작아짐  \n","\n","hi(1-hi) : sigmoid 함수의 미분  \n","0 < hi < 1 이므로 0< hi(1-hi) < 0.25 계속 값이 작아짐  \n","\n","### 결론  => sigmoid 함수에 문제가 있다.  \n","\n","Hilton 교수님이 ReLU를 고안하심 : 부동소숫점 연산이 없다. sigmoid는 exp라서 연산량이 많다.  \n","연산량도 줄고 비선형이다. ReLU(x) = max(0, x)   "]},{"cell_type":"markdown","metadata":{"id":"USURv_APnVQ7","colab_type":"text"},"source":["### 23쪽  \n","Saturation 현상이 발생 == 큰 값에서 기울기가 거의 0임  "]},{"cell_type":"markdown","metadata":{"id":"1QYz8K-TtR03","colab_type":"text"},"source":["### 24쪽\n","\n","drop out 대략적 원리  \n","== 몇 노드들이 빠져서 남은 노드들이 오차를 최소화 하기 위해서 학습을 해야함  \n","== 골고루 학습하게 됨\n","\n","---\n","\n","epsilon은 값이 작아져서 overflow하는 것을 막기 위해서... \n","\n","---\n","\n","사실 parameter수정할 때 모든 data를 고려해서 cost를 수정하면  \n","무조건 줄어드는 방향으로 나아간다. (convex하기 때문에, 공식에 의해서)  \n","\n","gpu 메모리가 한정적이고 계산속도를 개선하기 위해서 batch를 사용한다.   \n","batch를 사용해서 수정하면 cost가 커질수도 있다. (local하게 수정했기 때문이다.)  \n","이 오차를 해결하기 위해서 논문이 많이 나옴  \n","\n","batch_size가 너무 작으면 unstable함  \n","batch_size가 너무 크면 속도가 느림  \n","\n","parameter 수정을 batch 단위로 함  \n","parameter 갱신 수 = (데이터 수/batch_size) * epoch_size  \n","\n","---\n","\n","배치정규화  \n","batch(시험문제 일부분) 마다 정규화를 시켜서(난이도를 맞춰서)  \n","학습이 잘 되도록 하는 기법이라고 하셨는데 잘 모르겠다.  \n","batch마다의 데이터 분포를 맞추기 위해서 사용함 = 더 공부해보자...  \n","[배치정규화 쓰는 이유 1](http://dongyukang.com/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-%EB%85%BC%EB%AC%B8%EC%9D%84-%EC%9D%BD%EA%B3%A0/)  \n","[배치정규화 쓰는 이유 2](https://gomguard.tistory.com/186)  \n","[배치정규화 쓰는 이유 3](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)  \n","\n","---\n","\n","ADAM Optimizer는 다른 optimizer와 달리 방향성을 유지하면서(직선으로)  \n","최솟값에 대해서 나아간다. 휜 방향으로 나아가는 것보다는 빠를것 같다는 추측을 함!"]},{"cell_type":"markdown","metadata":{"id":"22nrDWk5y_6W","colab_type":"text"},"source":["### 25쪽  \n","새로운 데이터가 들어오면, feature를 계속 보강해야 함  \n","CNN에서 feature를 자동으로 추출해줌 "]},{"cell_type":"markdown","metadata":{"id":"E0d4oAvb5ETw","colab_type":"text"},"source":["### 29쪽  \n","한 장에 여러 필터 적용하는 이유 : \n","1. 한 장에서 다양한 image를 생성하기 위해서(한 장만으로 결정하기 힘들다)  \n","2. 원본 이미지는 복잡함 : filter를 거쳐서 이미지를 단순화시킴  (훼손시킴)\n","컴퓨터에게는 원본 이미지에 information이 너무 많고 복잡하다."]},{"cell_type":"markdown","metadata":{"id":"x4uvItbo6Iac","colab_type":"text"},"source":["### 32쪽  \n","max pooling layer는 이미지에서 뚜렷한 부분만 남기고 나머지는 제거하는 레이어로  \n","== 요약해서 학습시키면 효율이 좋다.  "]}]}