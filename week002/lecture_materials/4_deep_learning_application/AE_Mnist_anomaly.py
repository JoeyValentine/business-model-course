# -*- coding: utf-8 -*-
"""AE_Mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yTE8SBddV3yzwQ0kIDNVowIYtgRl5dmE
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from tensorflow.examples.tutorials.mnist import input_data

tf.reset_default_graph()
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

# parameters
learning_rate = 0.005
training_epochs = 15
batch_size = 100

# input place holders
X = tf.placeholder(tf.float32, [None, 784])
Y = tf.placeholder(tf.float32, [None, 784])

h1 = 128
h2 = 32
# weights & bias for nn layers
W1 = tf.get_variable("W1", shape=[784, h1], initializer=tf.contrib.layers.xavier_initializer())
b1 = tf.Variable(tf.random_normal([h1]))
L1 = tf.nn.relu(tf.matmul(X, W1) + b1)

W2 = tf.get_variable("W2", shape=[h1, h2], initializer=tf.contrib.layers.xavier_initializer())
b2 = tf.Variable(tf.random_normal([h2]))
L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)

W3 = tf.get_variable("W3", shape=[h2, h1], initializer=tf.contrib.layers.xavier_initializer())
b3 = tf.Variable(tf.random_normal([h1]))
L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)

W4 = tf.get_variable("W4", shape=[h1, 784], initializer=tf.contrib.layers.xavier_initializer())
b4 = tf.Variable(tf.random_normal([784]))
hypothesis =  tf.sigmoid(tf.matmul(L3, W4) + b4)

# define cost/loss & optimizer
cost = tf.reduce_mean(tf.square(hypothesis - Y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# initialize
sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)

# train my model
for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = int(mnist.train.num_examples / batch_size)

    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        idx = []
        for j in range(batch_size):
            if batch_ys[j][0] > 0:
                idx.append(j)
        batch_xs = np.delete(batch_xs, idx, axis=0)

        feed_dict = {X: batch_xs, Y: batch_xs}
        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)
        avg_cost += c / total_batch

    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))

print('Learning Finished!')

for i in range(5):
    imgNo = i
    aeResult = sess.run(hypothesis, feed_dict={X: mnist.test.images[imgNo:imgNo+1]})
    print(aeResult.shape)

    arr1 = np.array(mnist.test.images[imgNo])
    arr2 = np.array(aeResult)
    arr2 = (np.where(arr2 < 1.0e-01, 0, arr2))
    print(arr1, arr2)
    arr1.shape=(28,28)
    arr2.shape=(28,28)

    plt.subplot(211)
    plt.imshow(arr1)
    plt.subplot(212)
    plt.imshow(arr2)
    plt.show()

filename = './img/a.png'
image = mpimg.imread(filename)
image = image[:,:,0]
image = (np.where(image == 0, 1, 0))
image = image.flatten()
image = image.reshape(1,784)
aeResult = sess.run(hypothesis, feed_dict={X: image})
print(aeResult.shape)
arr1 = np.array(image)
arr2 = np.array(aeResult)
arr1.shape=(28,28)
arr2.shape=(28,28)
plt.subplot(211)
plt.imshow(arr1)
plt.subplot(212)
plt.imshow(arr2)
plt.show()

