{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"note006.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6rHgspJSSWMj","colab_type":"text"},"source":["### 1-11쪽\n","# 특징점으로만 구분하는데 문제가 많다.\n","ex) 특징은 유사한데,전혀 다른 이미지일 수 있다.  \n","=> 과거 통계학에선 feature를 최소화 했었다.  \n","=> but CNN은 픽셀 자체를 정보로 활용 == 픽셀 수만큼 feature로 사용 == 정확도 매우 높음"]},{"cell_type":"markdown","metadata":{"id":"1Mma1u0USWMj","colab_type":"text"},"source":["### 1-12쪽\n","과거, 컴퓨터 비전은 이미지의 특징점을 추출하는 방식으로 이미지의 **일부 정보**만 사용  \n","\n","ex) O, I 사진을 어떻게 구분할까?\n","\n","1. O 는 X의 편차와  Y의 편차가 유사하다.(데이터의 흩어진 정도)\n","2. I는  X편차보다 Y편차가 크다.\n","\n","근데, 이 기준(feature)만으론 I와 J로 구분이 안된다.  \n","따라서 모든 예외를 처리하기 위해서 feature를 더 고민해야 한다...  \n","문제마다 feature를 생성해야 한다. == 특징 추출은 어려운 과정임  \n","   \n","but, 딥러닝은 특징 추출을 자동으로 해줌 WOW"]},{"cell_type":"markdown","metadata":{"id":"-XlsDUEbSWMk","colab_type":"text"},"source":["### 1-13  \n","knn : 유유상종  \n","how sweet, how crunchy == feature  \n","적은 수의 feature만 사용하면 장점 == 메모리를 적게 사용 "]},{"cell_type":"markdown","metadata":{"id":"ckRsEP9ZSWMk","colab_type":"text"},"source":["### 1-14  \n","모든 입력변수는 양적변수이고 == 우편번호같은 변수는 평균에 의미가 없지..!\n","\n","표준편차가 중요함  \n","심사위원점수 : 700점 만점 (650 ~ 700)  \n","관객점수 : 300점 만점 (100 ~ 300)  \n","\n","관객점수가 더 중요해짐 == 표준편차를 고려해야 함  \n","\n","수능에서 같은 점수라도 표준편차에 따라서 성적이 다름  "]},{"cell_type":"markdown","metadata":{"id":"rubT-undSWMl","colab_type":"text"},"source":["### 1-16  \n","recall : 남자 여자 구분 AI에서, 남자가 다수일 때  \n","무조건 남자라고 구분하는 AI를 말하면 accuracy가 높아보임  \n","\n","실제 대회에서 F-measure를 사용함"]},{"cell_type":"markdown","metadata":{"id":"SqXAqCkwSWMl","colab_type":"text"},"source":["### 1-17\n","사전 정보에 의해서 의사결정  \n","\n","스펨 분류기 :  Naive Bayes 분류기   \n","비아그라라는 단어가 있을 때 스펨메일일 확률 == 매우 높다.    \n","\n","prior information : 사전 정보를 고려한 확률 == 더 정확  \n","\n","남녀 분류알고리즘에서 prior probability에서  \n","여자 비율을 곱하면 정확도가 더 정확하게 구해짐  \n","\n","likelihood : 경험한 확률, prior probability : 사전 정보  \n","2 가지 고려해서 의사결정...!  "]},{"cell_type":"markdown","metadata":{"id":"VdJ1wzv4SWMm","colab_type":"text"},"source":["### 1-18  \n","𝑃(𝑊1 ∩ ! 𝑊2 ∩ ! 𝑊3 ∩ 𝑊4|𝑆𝑝𝑎𝑚) : 확률 계산에서 𝑊𝑖들의 수가 많으면    \n","스팸 중 해당 경우의 Case가 작아 확률 계산이 부정확해 짐  \n",": 조건이 많아질수록 case가 작아서 분자가 작아짐  \n",": 확률이 부정확해짐   \n","\n","적은 데이터와 정확한 계산  \n","계산 공식은 완벽한데, 추정 오차가 많음  \n","\n","𝑃 (𝑊1|𝑠𝑝𝑎𝑚) 𝑃(! 𝑊2| 𝑠𝑝𝑎𝑚) 𝑃(! 𝑊3|𝑠𝑝𝑎𝑚) 𝑃(𝑊4| 𝑠𝑝𝑎𝑚) 𝑃(𝑠𝑝𝑎𝑚) :   \n","많은 데이터와 근사계산 중 후자를 선택함(Naive Bayes Algorithm)  \n","계산은 근사계산인데, 추정 오차가 적음  \n","\n","Naive Bayes에서는 W1, W2,... 가 독립임을 가정(그래서 Naive라고 부름)  \n","if A, B are independent, P(A∩B) = P(A) P(B)"]},{"cell_type":"markdown","metadata":{"id":"-SyR8HlqSWMm","colab_type":"text"},"source":["### 1-19  \n","분자만 계산해서 확률을 구한다. ham이나 spam인 경우 모두 분모가 같다.  \n","threshold값을 지정해서 특정 값을 넘어가면 스펨으로 분류"]},{"cell_type":"markdown","metadata":{"id":"liqFVnLOSWMm","colab_type":"text"},"source":["### 실습\n","jupyter notebook : interactive하게 test 가능  \n","pycharm 장점 : debugger 사용가능  \n","KNN : feature가 양적인 값일때만 사용할 수 있다.  \n","[GaussianNB](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) : 픽셀 값들이 정규 분포를 따른다고 가정해서 KNN보다 성능이 떨어진다.  \n","==> f(A∩B) = f(A) f(B) : P가 아니라 f로 바꾼다고 하셨는데...  \n","\n","---\n","# GaussianNB에서 정규분포를 가정하는 이유\n","인구에서 키 190이상 비율?\n","1. 진짜 많이 sampling해서 비율 구하기 \n","-> 정확하긴한데, 샘플링에 돈이 많이 듦  \n","-> 조금 샘플링하면 190이상 한 명도 안나올 수 있음  \n","2. 키190인 사람이 1명이 나올때까지 추출해서 그것을 비율로 사용\n","3. 키가 정규분포를 따른다고 가정하면,\n","적은 샘플링으로 평균과 분산을 구한 다음, 정규분포 그래프에서 190이상인 사람의 비율을 구한다.\n","\n","장점 : 샘플링을 1번보다 조금해도 된다.\n","단점 : 가정(키가 정규분포를 따른다)이 틀리면 오차가 크다.\n","\n","스몰 데이터의 경우는(과거 통계학 관점) 분포의 가정을 하면 더 유리함 -> 데이터에서 분포를 찾기 힘드니까...  \n","빅 데이터의 경우에는 데이터에서 분포를 확인할 수 있으므로, 가정하는 것이 불리함(오히려 오차가 커짐)"]},{"cell_type":"markdown","metadata":{"id":"9o9798nbSWMn","colab_type":"text"},"source":["### 1-21  \n","불확실할 때가 가장 선택하기 힘들다.  \n","확실할 때 의사결정이 편해진다.  \n","ex)  \n","취직률이 99퍼면 물건을 사기로 마음을 먹게됨  \n","취직률이 10퍼면 물건을 안사기로 결심함  \n","취직률이 50퍼면 몰라서 고민하게 됨  \n","\n","\n","entropy를 낮추는 방향으로 향하면 의사결정이 편해진다.   "]},{"cell_type":"markdown","metadata":{"id":"1PioS5eGSWMn","colab_type":"text"},"source":["### 1-23\n","시행을 많이 했을 때, 특정 수가 뽑힐 확률 거의 0.634  \n","\n","[Random forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)에서 def [boostrap](https://www.quantstart.com/articles/bootstrap-aggregation-random-forests-and-boosted-trees)(x) 쓰는데...  \n","Robust한 모델이다 : 데이터 변화에 많이 흔들리지 않는 모델을 생성 가능  \n","\n","학습 과정에서 소외되는 feature가 있음    \n","학습 과정에서 소외되는 feature가 있어서  \n","대부분 많은 feature를 학습에 참여시키기 위해서 일부러 몇 data를 탈락시킴"]}]}